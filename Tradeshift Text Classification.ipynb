{
 "metadata": {
  "name": "",
  "signature": "sha256:a28b059c4f437b102b0713705f62b72e1e450a84501252ac379ba425e7d881ff"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Apache Spark Machine Learning\n",
      "This Notebook aims to demonstrate the workflow of conducting text analysis, by utilizing Apache Spark."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os, sys\n",
      "from numpy import array\n",
      "''' ML Module '''\n",
      "import nltk\n",
      "''' Apache PySpark Module '''\n",
      "from pyspark import SparkConf, SparkContext\n",
      "from pyspark.mllib import classification, regression\n",
      "''' AWS Python API Module '''\n",
      "from boto.s3.connection import S3Connection\n",
      "from boto.s3.key import Key\n",
      "''' AWS Account Access Key '''\n",
      "from aws_keypair import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Spark Configuration '''\n",
      "conf = (SparkConf()\n",
      "        .setMaster(\"local\")\n",
      "        .setAppName(\"MLSpark\")\n",
      "        .setSparkHome(\"/Users/beingzy/Documents/kaggle/ttc\")\n",
      "        .set(\"spark.excutor.memory\", \"1g\"))\n",
      "\n",
      "sc = SparkContext()\n",
      "\n",
      "''' AWS S3 connection '''\n",
      "# print aws_key.access_key_id\n",
      "# print aws_key.secret_access_key\n",
      "s3_conn = S3Connection(aws_key.access_key_id, aws_key.secret_access_key)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-61-31326dea80af>:8 ",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-108-cae29afd2ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         .set(\"spark.excutor.memory\", \"1g\"))\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m''' AWS S3 connection '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/beingzy/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mtempNamedTuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Callsite\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"function file linenum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempNamedTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinenum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
        "\u001b[0;32m/Users/beingzy/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 228\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-61-31326dea80af>:8 "
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Data Process\n",
      "# s3_bucket = s3_conn.create_bucket('kaggle_ttc')\n",
      "# s3_bucket = s3_conn.get_bucket('kaggle_ttc')\n",
      "# print \"trainLabel data file's size: %d MB\" % ( os.stat(os.getcwd() + '/data/trainLabels').st_size )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Natural Languge Process\n",
      "data = sc.textFile('/Users/beingzy/spark/data/mllib/ridge-data/lpsa.data')\n",
      "parsedData = data.map(lambda line: array([float(x) for x in line.replace(',', ' ').split(' ')]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parsedData.take(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 112,
       "text": [
        "[array([-0.4307829 , -1.63735563, -2.00621178, -1.86242597, -1.0247058 ,\n",
        "        -0.52294089, -0.86317119, -1.04215729, -0.86446651]),\n",
        " array([-0.1625189 , -1.98898046, -0.72200876, -0.78789619, -1.0247058 ,\n",
        "        -0.52294089, -0.86317119, -1.04215729, -0.86446651]),\n",
        " array([-0.1625189 , -1.57881888, -2.18878403,  1.36116337, -1.0247058 ,\n",
        "        -0.52294089, -0.86317119,  0.34262705, -0.1553481 ]),\n",
        " array([-0.1625189 , -2.16691708, -0.8079939 , -0.78789619, -1.0247058 ,\n",
        "        -0.52294089, -0.86317119, -1.04215729, -0.86446651]),\n",
        " array([ 0.3715636 , -0.50787448, -0.45883405, -0.2506313 , -1.0247058 ,\n",
        "        -0.52294089, -0.86317119, -1.04215729, -0.86446651])]"
       ]
      }
     ],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeling[\"ax\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 84,
       "text": [
        "'val'"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}